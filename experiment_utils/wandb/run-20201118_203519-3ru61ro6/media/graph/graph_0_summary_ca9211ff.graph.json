{"format": "torch", "nodes": [{"name": "transformer", "id": 139744431540936, "class_name": "GPT2Model(\n  (wte): Embedding(50257, 768)\n  (wpe): Embedding(1024, 768)\n  (drop): Dropout(p=0.1, inplace=False)\n  (h): ModuleList(\n    (0): Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (1): Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (2): Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (3): Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (4): Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (5): Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (6): Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (7): Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (8): Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (9): Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (10): Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (11): Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n)", "parameters": [["wte.weight", [50257, 768]], ["wpe.weight", [1024, 768]], ["h.0.ln_1.weight", [768]], ["h.0.ln_1.bias", [768]], ["h.0.attn.c_attn.weight", [768, 2304]], ["h.0.attn.c_attn.bias", [2304]], ["h.0.attn.c_proj.weight", [768, 768]], ["h.0.attn.c_proj.bias", [768]], ["h.0.ln_2.weight", [768]], ["h.0.ln_2.bias", [768]], ["h.0.mlp.c_fc.weight", [768, 3072]], ["h.0.mlp.c_fc.bias", [3072]], ["h.0.mlp.c_proj.weight", [3072, 768]], ["h.0.mlp.c_proj.bias", [768]], ["h.1.ln_1.weight", [768]], ["h.1.ln_1.bias", [768]], ["h.1.attn.c_attn.weight", [768, 2304]], ["h.1.attn.c_attn.bias", [2304]], ["h.1.attn.c_proj.weight", [768, 768]], ["h.1.attn.c_proj.bias", [768]], ["h.1.ln_2.weight", [768]], ["h.1.ln_2.bias", [768]], ["h.1.mlp.c_fc.weight", [768, 3072]], ["h.1.mlp.c_fc.bias", [3072]], ["h.1.mlp.c_proj.weight", [3072, 768]], ["h.1.mlp.c_proj.bias", [768]], ["h.2.ln_1.weight", [768]], ["h.2.ln_1.bias", [768]], ["h.2.attn.c_attn.weight", [768, 2304]], ["h.2.attn.c_attn.bias", [2304]], ["h.2.attn.c_proj.weight", [768, 768]], ["h.2.attn.c_proj.bias", [768]], ["h.2.ln_2.weight", [768]], ["h.2.ln_2.bias", [768]], ["h.2.mlp.c_fc.weight", [768, 3072]], ["h.2.mlp.c_fc.bias", [3072]], ["h.2.mlp.c_proj.weight", [3072, 768]], ["h.2.mlp.c_proj.bias", [768]], ["h.3.ln_1.weight", [768]], ["h.3.ln_1.bias", [768]], ["h.3.attn.c_attn.weight", [768, 2304]], ["h.3.attn.c_attn.bias", [2304]], ["h.3.attn.c_proj.weight", [768, 768]], ["h.3.attn.c_proj.bias", [768]], ["h.3.ln_2.weight", [768]], ["h.3.ln_2.bias", [768]], ["h.3.mlp.c_fc.weight", [768, 3072]], ["h.3.mlp.c_fc.bias", [3072]], ["h.3.mlp.c_proj.weight", [3072, 768]], ["h.3.mlp.c_proj.bias", [768]], ["h.4.ln_1.weight", [768]], ["h.4.ln_1.bias", [768]], ["h.4.attn.c_attn.weight", [768, 2304]], ["h.4.attn.c_attn.bias", [2304]], ["h.4.attn.c_proj.weight", [768, 768]], ["h.4.attn.c_proj.bias", [768]], ["h.4.ln_2.weight", [768]], ["h.4.ln_2.bias", [768]], ["h.4.mlp.c_fc.weight", [768, 3072]], ["h.4.mlp.c_fc.bias", [3072]], ["h.4.mlp.c_proj.weight", [3072, 768]], ["h.4.mlp.c_proj.bias", [768]], ["h.5.ln_1.weight", [768]], ["h.5.ln_1.bias", [768]], ["h.5.attn.c_attn.weight", [768, 2304]], ["h.5.attn.c_attn.bias", [2304]], ["h.5.attn.c_proj.weight", [768, 768]], ["h.5.attn.c_proj.bias", [768]], ["h.5.ln_2.weight", [768]], ["h.5.ln_2.bias", [768]], ["h.5.mlp.c_fc.weight", [768, 3072]], ["h.5.mlp.c_fc.bias", [3072]], ["h.5.mlp.c_proj.weight", [3072, 768]], ["h.5.mlp.c_proj.bias", [768]], ["h.6.ln_1.weight", [768]], ["h.6.ln_1.bias", [768]], ["h.6.attn.c_attn.weight", [768, 2304]], ["h.6.attn.c_attn.bias", [2304]], ["h.6.attn.c_proj.weight", [768, 768]], ["h.6.attn.c_proj.bias", [768]], ["h.6.ln_2.weight", [768]], ["h.6.ln_2.bias", [768]], ["h.6.mlp.c_fc.weight", [768, 3072]], ["h.6.mlp.c_fc.bias", [3072]], ["h.6.mlp.c_proj.weight", [3072, 768]], ["h.6.mlp.c_proj.bias", [768]], ["h.7.ln_1.weight", [768]], ["h.7.ln_1.bias", [768]], ["h.7.attn.c_attn.weight", [768, 2304]], ["h.7.attn.c_attn.bias", [2304]], ["h.7.attn.c_proj.weight", [768, 768]], ["h.7.attn.c_proj.bias", [768]], ["h.7.ln_2.weight", [768]], ["h.7.ln_2.bias", [768]], ["h.7.mlp.c_fc.weight", [768, 3072]], ["h.7.mlp.c_fc.bias", [3072]], ["h.7.mlp.c_proj.weight", [3072, 768]], ["h.7.mlp.c_proj.bias", [768]], ["h.8.ln_1.weight", [768]], ["h.8.ln_1.bias", [768]], ["h.8.attn.c_attn.weight", [768, 2304]], ["h.8.attn.c_attn.bias", [2304]], ["h.8.attn.c_proj.weight", [768, 768]], ["h.8.attn.c_proj.bias", [768]], ["h.8.ln_2.weight", [768]], ["h.8.ln_2.bias", [768]], ["h.8.mlp.c_fc.weight", [768, 3072]], ["h.8.mlp.c_fc.bias", [3072]], ["h.8.mlp.c_proj.weight", [3072, 768]], ["h.8.mlp.c_proj.bias", [768]], ["h.9.ln_1.weight", [768]], ["h.9.ln_1.bias", [768]], ["h.9.attn.c_attn.weight", [768, 2304]], ["h.9.attn.c_attn.bias", [2304]], ["h.9.attn.c_proj.weight", [768, 768]], ["h.9.attn.c_proj.bias", [768]], ["h.9.ln_2.weight", [768]], ["h.9.ln_2.bias", [768]], ["h.9.mlp.c_fc.weight", [768, 3072]], ["h.9.mlp.c_fc.bias", [3072]], ["h.9.mlp.c_proj.weight", [3072, 768]], ["h.9.mlp.c_proj.bias", [768]], ["h.10.ln_1.weight", [768]], ["h.10.ln_1.bias", [768]], ["h.10.attn.c_attn.weight", [768, 2304]], ["h.10.attn.c_attn.bias", [2304]], ["h.10.attn.c_proj.weight", [768, 768]], ["h.10.attn.c_proj.bias", [768]], ["h.10.ln_2.weight", [768]], ["h.10.ln_2.bias", [768]], ["h.10.mlp.c_fc.weight", [768, 3072]], ["h.10.mlp.c_fc.bias", [3072]], ["h.10.mlp.c_proj.weight", [3072, 768]], ["h.10.mlp.c_proj.bias", [768]], ["h.11.ln_1.weight", [768]], ["h.11.ln_1.bias", [768]], ["h.11.attn.c_attn.weight", [768, 2304]], ["h.11.attn.c_attn.bias", [2304]], ["h.11.attn.c_proj.weight", [768, 768]], ["h.11.attn.c_proj.bias", [768]], ["h.11.ln_2.weight", [768]], ["h.11.ln_2.bias", [768]], ["h.11.mlp.c_fc.weight", [768, 3072]], ["h.11.mlp.c_fc.bias", [3072]], ["h.11.mlp.c_proj.weight", [3072, 768]], ["h.11.mlp.c_proj.bias", [768]], ["ln_f.weight", [768]], ["ln_f.bias", [768]]], "output_shape": [[1, 1024, 768], [[2, 1, 12, 1024, 64], [2, 1, 12, 1024, 64], [2, 1, 12, 1024, 64], [2, 1, 12, 1024, 64], [2, 1, 12, 1024, 64], [2, 1, 12, 1024, 64], [2, 1, 12, 1024, 64], [2, 1, 12, 1024, 64], [2, 1, 12, 1024, 64], [2, 1, 12, 1024, 64], [2, 1, 12, 1024, 64], [2, 1, 12, 1024, 64]]], "num_parameters": [38597376, 786432, 768, 768, 1769472, 2304, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 1769472, 2304, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 1769472, 2304, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 1769472, 2304, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 1769472, 2304, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 1769472, 2304, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 1769472, 2304, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 1769472, 2304, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 1769472, 2304, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 1769472, 2304, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 1769472, 2304, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 1769472, 2304, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768]}, {"name": "lm_head", "id": 139743406392432, "class_name": "Linear(in_features=768, out_features=50257, bias=False)", "parameters": [["weight", [50257, 768]]], "output_shape": [[1, 1024, 50257]], "num_parameters": [38597376]}], "edges": []}